{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMMaEdtiwRbXgdzdQiflme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinVerma2027/PRML-Apr2025/blob/main/ml_ipynb_files/svm_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Œ Support Vector Machines"
      ],
      "metadata": {
        "id": "syVCVI_8jNbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Problem  \n",
        "We are working on classifying handwritten digits using the MNIST dataset. Each image in this dataset is 28x28 pixels, and the data is provided in CSV format, where each row represents a flattened grayscale image.  \n",
        "\n",
        "## Dataset Overview  \n",
        "- **Size:** 60,000 rows Ã— 785 columns  \n",
        "- **First Column:** The label (y), representing digits from 0 to 9  \n",
        "- **Next 784 Columns:** Pixel values (X), indicating grayscale intensities  \n",
        "- **Pixel Range:** 0 to 255 (normalized to 0-1 for better model performance)  \n",
        "\n",
        "For this task, we will train a Support Vector Machine (SVM) using Stochastic Gradient Descent (SGD), which is well-suited for large datasets like MNIST.\n"
      ],
      "metadata": {
        "id": "rREnnCkljWAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "wZoLrErMcGoG",
        "outputId": "92762552-65e4-41ad-c707-f42caa8cee64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gj1rEzLDzNBpRswfqZaJxCFs5EwQSA87\n",
            "From (redirected): https://drive.google.com/uc?id=1gj1rEzLDzNBpRswfqZaJxCFs5EwQSA87&confirm=t&uuid=356d8d58-64f1-472d-92ed-ecb90100d99a\n",
            "To: /content/train_dataset.csv\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110M/110M [00:01<00:00, 78.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MX9ckuU6rhL3PaVvflrXsZ-0m2bFeWsy\n",
            "To: /content/test_dataset.csv\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.3M/18.3M [00:00<00:00, 42.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
            "0    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "2    4    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "3    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "4    9    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
            "\n",
            "   779  780  781  782  783  784  \n",
            "0    0    0    0    0    0    0  \n",
            "1    0    0    0    0    0    0  \n",
            "2    0    0    0    0    0    0  \n",
            "3    0    0    0    0    0    0  \n",
            "4    0    0    0    0    0    0  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "Training data shape: (60000, 784)\n",
            "Test data shape: (10000, 784)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# file import from derive\n",
        "train_file_id = \"1gj1rEzLDzNBpRswfqZaJxCFs5EwQSA87\"\n",
        "test_file_id = \"1MX9ckuU6rhL3PaVvflrXsZ-0m2bFeWsy\"\n",
        "\n",
        "train_url = f\"https://drive.google.com/uc?id={train_file_id}\"\n",
        "test_url = f\"https://drive.google.com/uc?id={test_file_id}\"\n",
        "\n",
        "\n",
        "# downloading file\n",
        "train_output = \"train_dataset.csv\"\n",
        "test_output = \"test_dataset.csv\"\n",
        "\n",
        "gdown.download(train_url, train_output, quiet=False)\n",
        "gdown.download(test_url, test_output, quiet=False)\n",
        "\n",
        "\n",
        "# dataframe formation\n",
        "train_df = pd.read_csv(train_output, header=None)\n",
        "test_df = pd.read_csv(test_output, header=None)\n",
        "\n",
        "\n",
        "print(train_df.head())\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "# making y = label(1st column) and x = row (other column).\n",
        "\n",
        "#for train dataset\n",
        "y_train = train_df.iloc[:, 0].values  # Labels\n",
        "X_train = train_df.iloc[:, 1:].values  # Features (pixel values)\n",
        "\n",
        "#for test dataset\n",
        "y_test = test_df.iloc[:, 0].values  # Labels\n",
        "X_test = test_df.iloc[:, 1:].values  # Features (pixel values)\n",
        "\n",
        "# normalize pixel values (0-255 â†’ 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# shape of train and test\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "# displaying some images.|\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.set_title(f\"Label: {y_train[i]}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Custom Multi-Class Support Vector Machine (SVM) with Stochastic Gradient Descent (SGD)\n",
        "\n",
        "This implementation aims to find optimal hyperplanes that separate different classes for multi-class classification, specifically tailored for the MNIST dataset with 10 classes. It utilizes a One-vs-All (OvA) strategy.\n",
        "\n",
        "ðŸ”¹ Why Custom SGD?\n",
        "* **Fine-grained Control:** Allows for direct manipulation of learning rates, regularization, and batch sizes.\n",
        "* **Educational Purpose:** Provides a deeper understanding of the SVM training process with SGD.\n",
        "* **Efficiency for Large Datasets:** Handles the 60k samples of MNIST effectively by updating weights iteratively.\n",
        "\n",
        "âš™ Model Training:\n",
        "We implement a custom `FastMultiClassSVM` class that:\n",
        "\n",
        "* Uses the hinge loss function, inherent to SVM.\n",
        "* Applies L2 regularization (lambda_param) to prevent overfitting.\n",
        "* Trains with mini-batch SGD for efficient learning.\n",
        "* Calculates and stores the total cost (regularized hinge loss) during training.\n",
        "\n",
        "The provided code then trains the model with different learning rates to observe their impact on performance."
      ],
      "metadata": {
        "id": "pxnJwFW449uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "class FastMultiClassSVM:\n",
        "    def __init__(self, learning_rate=0.015, lambda_param=0.008, epochs=750, batch_size=256):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.classes = None\n",
        "        self.models = {}  # Dictionary to store (W, b) for each class\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train SVM using Mini-Batch SGD (One-vs-All).\"\"\"\n",
        "        self.classes = np.unique(y)  # Unique digit labels (0-9)\n",
        "        m, n = X.shape  # Samples, Features\n",
        "\n",
        "        for digit in self.classes:\n",
        "            print(f\"Training for digit {digit}...\")\n",
        "            y_binary = np.where(y == digit, 1, -1)  # Convert labels to {+1, -1}\n",
        "\n",
        "            W = np.zeros(n)  # Initialize weights\n",
        "            b = 0  # Initialize bias\n",
        "\n",
        "            for epoch in range(self.epochs):\n",
        "                indices = np.random.permutation(m)  # Shuffle data each epoch\n",
        "                X_shuffled, y_shuffled = X[indices], y_binary[indices]\n",
        "\n",
        "                for batch_start in range(0, m, self.batch_size):\n",
        "                    batch_end = batch_start + self.batch_size\n",
        "                    X_batch = X_shuffled[batch_start:batch_end]\n",
        "                    y_batch = y_shuffled[batch_start:batch_end]\n",
        "\n",
        "                    # Compute margins for the batch\n",
        "                    margins = y_batch * (np.dot(X_batch, W) + b)\n",
        "                    misclassified = margins < 1  # Boolean mask for misclassified points\n",
        "\n",
        "                    # Compute gradients\n",
        "                    W_gradient = 2 * self.lambda_param * W - np.dot(y_batch[misclassified], X_batch[misclassified])\n",
        "                    b_gradient = -np.sum(y_batch[misclassified])\n",
        "\n",
        "                    # Update weights\n",
        "                    W -= self.learning_rate * W_gradient / self.batch_size\n",
        "                    b -= self.learning_rate * b_gradient / self.batch_size\n",
        "\n",
        "                if epoch % 100 == 0:\n",
        "                    print(f\"Epoch {epoch}/{self.epochs} completed\")\n",
        "\n",
        "            self.models[digit] = (W, b)  # Store trained model\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels using One-vs-All strategy.\"\"\"\n",
        "        scores = np.zeros((X.shape[0], len(self.classes)))  # Store decision scores\n",
        "\n",
        "        for i, digit in enumerate(self.classes):\n",
        "            W, b = self.models[digit]\n",
        "            scores[:, i] = np.dot(X, W) + b  # Compute scores for each class\n",
        "\n",
        "        return self.classes[np.argmax(scores, axis=1)]  # Pick class with highest score\n"
      ],
      "metadata": {
        "id": "DnBv4uxFcXik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & Train SVM\n",
        "fast_svm = FastMultiClassSVM(learning_rate=0.015, lambda_param=0.008, epochs=750, batch_size=256)\n",
        "fast_svm.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFjbq2B3OV-K",
        "outputId": "4d701816-f935-43db-f465-ed50f8bba234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for digit 0...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 1...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 2...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 3...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 4...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 5...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 6...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 7...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 8...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n",
            "Training for digit 9...\n",
            "Epoch 0/750 completed\n",
            "Epoch 100/750 completed\n",
            "Epoch 200/750 completed\n",
            "Epoch 300/750 completed\n",
            "Epoch 400/750 completed\n",
            "Epoch 500/750 completed\n",
            "Epoch 600/750 completed\n",
            "Epoch 700/750 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "y_pred = fast_svm.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Updated Custom SVM Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "0l0uSGIIpo_w",
        "outputId": "271199a4-018d-449c-ee33-4909cc9f752b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Custom SVM Accuracy: 0.9223\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       980\n",
            "           1       0.97      0.98      0.97      1135\n",
            "           2       0.93      0.90      0.92      1032\n",
            "           3       0.91      0.91      0.91      1010\n",
            "           4       0.91      0.93      0.92       982\n",
            "           5       0.89      0.87      0.88       892\n",
            "           6       0.93      0.95      0.94       958\n",
            "           7       0.93      0.92      0.93      1028\n",
            "           8       0.88      0.87      0.88       974\n",
            "           9       0.91      0.89      0.90      1009\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Cost Function: Hinge Loss\n",
        "\n",
        "SVMs use the **hinge loss function** to maximize the margin between classes:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{w}) = \\sum_{i=1}^{N} \\max(0, 1 - y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b)) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ y_i $ is the true label.\n",
        "- $ \\mathbf{x}_i $ is the input feature vector.\n",
        "- $ \\mathbf{w} $ is the weight vector.\n",
        "- $ b $ is the bias term.\n",
        "- $ \\lambda $ is the regularization parameter.\n"
      ],
      "metadata": {
        "id": "2fwjqKCd5BRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Precision, Recall, and F1-Score\n",
        "\n",
        "We evaluate the model using the following metrics:\n",
        "\n",
        "- **Precision** ($ P $): How many predicted digits were correct?\n",
        "  $$\n",
        "  P = \\frac{TP}{TP + FP}\n",
        "  $$\n",
        "\n",
        "- **Recall** ($ R $): How many actual digits were correctly predicted?\n",
        "  $$\n",
        "  R = \\frac{TP}{TP + FN}\n",
        "  $$\n",
        "\n",
        "- **F1-score**: A balance between precision and recall.\n",
        "  $$\n",
        "  F1 = 2 \\times \\frac{P \\times R}{P + R}\n",
        "  $$\n"
      ],
      "metadata": {
        "id": "i4N8Nk_n6crK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion: Multi-Class SVM with Mini-Batch SGD on MNIST**  \n",
        "\n",
        "### **Key Takeaways**  \n",
        "This study implemented a **Multi-Class Support Vector Machine (SVM)** for MNIST digit classification using the **One-vs-All (OvA) strategy**. Training was performed using **Mini-Batch Stochastic Gradient Descent (SGD)**, which significantly improved convergence speed and accuracy. By optimizing the **learning rate (0.015), regularization parameter (0.008), and batch size (256)**, the final test accuracy was restored to approximately **91-92%**, addressing the earlier drop caused by full-batch updates.  \n",
        "\n",
        "### **Why Mini-Batch SGD Improved Performance?**  \n",
        "1. **Faster Convergence** â€“ Weight updates occur after each mini-batch instead of waiting for the entire dataset.  \n",
        "2. **Better Generalization** â€“ Mini-Batch SGD helps escape sharp local minima and reduces overfitting.  \n",
        "3. **Scalability** â€“ Handles large datasets like MNIST (60K samples) efficiently compared to full-batch training.  \n",
        "\n",
        "### **Future Enhancements**  \n",
        "- **Adaptive Learning Rates** â€“ Methods such as Adam or Momentum-SGD could be explored to further optimize convergence.  \n",
        "- **Kernel SVM** â€“ Adding non-linear kernels may improve classification for more complex datasets.  \n",
        "- **Comparison with CNNs** â€“ Deep learning models like Convolutional Neural Networks (CNNs) typically outperform SVMs on image classification tasks and could be evaluated as an alternative approach.  \n",
        "\n",
        "### **Final Thoughts**  \n",
        "This optimized implementation successfully achieves **high classification accuracy** while maintaining computational efficiency. The use of **Mini-Batch SGD** proves to be an effective approach for large-scale SVM training, making it applicable to other machine learning problems involving high-dimensional data."
      ],
      "metadata": {
        "id": "dR4Rl8IH677F"
      }
    }
  ]
}